"""
Delphi LightRAG Simple Version (Dockerãªã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨)
"""
import os
import asyncio
from typing import List, Dict, Any
import numpy as np
from tree_sitter import Node, Parser, Language
import tree_sitter_pascal as tspascal
import openai
from pathlib import Path

# ç°¡æ˜“çš„ãªRAGå®Ÿè£…ï¼ˆLightRAGã‚’ä½¿ã‚ãªã„ï¼‰
class SimpleDelphiRAG:
    def __init__(self, working_dir="./simple_rag_storage"):
        self.working_dir = Path(working_dir)
        self.working_dir.mkdir(exist_ok=True)
        
        # Tree-sitterè¨­å®š
        self.pascal_lang = Language(tspascal.language())
        self.parser = Parser(self.pascal_lang)
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.chunks = []
        self.entities = []
        self.embeddings = []
        
        # OpenAIè¨­å®š
        self.client = openai.AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
    async def analyze_delphi_code(self, file_path: str):
        """Delphiã‚³ãƒ¼ãƒ‰ã‚’è§£æ"""
        with open(file_path, 'rb') as f:
            content = f.read()
            
        # ASTè§£æ
        tree = self.parser.parse(content)
        root_node = tree.root_node
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        entities = self._extract_entities(root_node, content)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ä¿å­˜
        text_content = content.decode('utf-8')
        chunk = {
            'file': os.path.basename(file_path),
            'content': text_content,
            'entities': entities
        }
        
        # åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        embedding = await self._embed_text(text_content)
        
        self.chunks.append(chunk)
        self.entities.extend(entities)
        self.embeddings.append(embedding)
        
        print(f"âœ… Analyzed {file_path}: Found {len(entities)} entities")
        
    def _extract_entities(self, node: Node, content: bytes, entities=None):
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º"""
        if entities is None:
            entities = []
            
        # å®šç¾©ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—
        definition_types = {
            "class_type": "class",
            "interface_type": "interface", 
            "function_declaration": "function",
            "procedure_declaration": "procedure",
            "method_declaration": "method"
        }
        
        if node.type in definition_types:
            # åå‰ã‚’æ¢ã™
            for child in node.children:
                if child.type == "identifier":
                    name = content[child.start_byte:child.end_byte].decode('utf-8')
                    entities.append({
                        'name': name,
                        'type': definition_types[node.type],
                        'line': node.start_point[0]
                    })
                    break
        
        # å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«æ¢ç´¢
        for child in node.children:
            self._extract_entities(child, content, entities)
            
        return entities
    
    async def _embed_text(self, text: str) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        response = await self.client.embeddings.create(
            model="text-embedding-3-large",
            input=text[:8000]  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
        )
        return response.data[0].embedding
    
    async def query(self, question: str) -> str:
        """è³ªå•ã«å›ç­”"""
        if not self.chunks:
            return "No code has been analyzed yet."
        
        # è³ªå•ã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
        query_embedding = await self._embed_text(question)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’æ¢ã™
        similarities = []
        for i, chunk_embedding in enumerate(self.embeddings):
            similarity = np.dot(query_embedding, chunk_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)
            )
            similarities.append((similarity, i))
        
        # æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
        similarities.sort(reverse=True)
        best_chunk = self.chunks[similarities[0][1]]
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        entities_info = "\n".join([
            f"- {e['type']} {e['name']} (line {e['line']})"
            for e in best_chunk['entities']
        ])
        
        # GPTã§å›ç­”ã‚’ç”Ÿæˆ
        prompt = f"""
        Based on the following Delphi code analysis, answer the question.
        
        File: {best_chunk['file']}
        
        Entities found:
        {entities_info}
        
        Code excerpt:
        {best_chunk['content'][:2000]}...
        
        Question: {question}
        
        Please answer in Japanese.
        """
        
        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful Delphi code analysis assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content


# ä½¿ç”¨ä¾‹
async def main():
    print("ğŸš€ Simple Delphi RAG Demo (No Docker required)")
    print("=" * 50)
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ Error: OPENAI_API_KEY not set!")
        print("Please set: export OPENAI_API_KEY='your-api-key'")
        return
    
    # Simple RAGã‚’åˆæœŸåŒ–
    rag = SimpleDelphiRAG()
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’è§£æ
    if os.path.exists("sample_delphi_code.pas"):
        await rag.analyze_delphi_code("sample_delphi_code.pas")
        
        # è³ªå•ä¾‹
        queries = [
            "TSampleClassã«ã¯ã©ã®ã‚ˆã†ãªãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "ã“ã®ã‚³ãƒ¼ãƒ‰ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        ]
        
        for query in queries:
            print(f"\nğŸ’¬ Q: {query}")
            answer = await rag.query(query)
            print(f"ğŸ“ A: {answer}")
            print("-" * 50)
    else:
        print("âŒ sample_delphi_code.pas not found!")

if __name__ == "__main__":
    asyncio.run(main())